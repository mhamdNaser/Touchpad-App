# app/services/prediction_pipeline.py
import os
import torch
import numpy as np
from typing import Dict, Any, List
from joblib import load
import logging

logger = logging.getLogger(__name__)

class CompatibleEncoder(torch.nn.Module):
    def __init__(self, input_dim=60*20*6, latent_dim=128):
        super().__init__()
        # Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø·Ø£ØŒ ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ BatchNorm ÙˆØ·Ø¨Ù‚Ø§Øª Ø£ÙƒØ«Ø±
        self.net = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 512),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(512),  # Ø£Ø¶ÙÙ†Ø§ BatchNorm
            torch.nn.Linear(512, 256),
            torch.nn.ReLU(), 
            torch.nn.BatchNorm1d(256),  # Ø£Ø¶ÙÙ†Ø§ BatchNorm
            torch.nn.Linear(256, latent_dim)
        )
    
    def forward(self, x):
        # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ ØµØ­ÙŠØ­Ø©
        if x.shape[1] != self.net[0].in_features:
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ù…Ø®ØªÙ„ÙØ©ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§
            if x.shape[1] > self.net[0].in_features:
                x = x[:, :self.net[0].in_features]
            else:
                padding = torch.zeros(x.shape[0], self.net[0].in_features - x.shape[1])
                x = torch.cat([x, padding], dim=1)
        return self.net(x)

class FlexibleEncoder(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # Ø³Ù†Ø¨Ù†ÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ state_dict
        self.layers = torch.nn.ModuleList()
        
    def build_from_state_dict(self, state_dict, input_dim):
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹ Ù…Ù† state_dict"""
        print("ğŸ”¨ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹ Ù…Ù† state_dict...")
        
        # ØªØ­Ù„ÙŠÙ„ state_dict Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¨Ù†ÙŠØ©
        linear_layers = {}
        bn_layers = {}
        
        for key, tensor in state_dict.items():
            if 'weight' in key and 'bn' not in key and 'batch' not in key:
                layer_name = key.replace('.weight', '')
                linear_layers[layer_name] = tensor.shape
            elif 'bias' in key and 'bn' not in key and 'batch' not in key:
                layer_name = key.replace('.bias', '')
            elif 'weight' in key and ('bn' in key or 'batch' in key):
                layer_name = key.replace('.weight', '')
                bn_layers[layer_name] = tensor.shape
        
        print(f"ğŸ“Š Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø®Ø·ÙŠØ©: {linear_layers}")
        print(f"ğŸ“Š Ø·Ø¨Ù‚Ø§Øª BatchNorm: {bn_layers}")
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        layers = []
        current_dim = input_dim
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­
        sorted_keys = sorted(linear_layers.keys())
        for i, key in enumerate(sorted_keys):
            out_dim = linear_layers[key][0]
            
            # Ø¥Ø¶Ø§ÙØ© Ø·Ø¨Ù‚Ø© Ø®Ø·ÙŠØ©
            layers.append(torch.nn.Linear(current_dim, out_dim))
            layers.append(torch.nn.ReLU())
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ BatchNorm Ø¨Ø¹Ø¯ Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø¨Ù‚Ø©
            bn_key = f"net.{2*i+1}" if 'net' in key else f"{key.replace('net.', '')}_bn"
            if bn_key in bn_layers:
                layers.append(torch.nn.BatchNorm1d(out_dim))
            
            current_dim = out_dim
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© (latent)
        layers.append(torch.nn.Linear(current_dim, list(linear_layers.values())[-1][0]))
        
        self.net = torch.nn.Sequential(*layers)
        print(f"âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¨Ù†ÙŠ: {self.net}")
        
    def forward(self, x):
        return self.net(x)

class ClusteringPredictionPipeline:
    def __init__(
        self,
        encoder_path: str = "artifacts/encoder.pth",
        kmeans_path: str = "artifacts/gesture_kmeans.joblib", 
        mapping_path: str = "artifacts/gesture_mapping.joblib",
        max_frames: int = 60,
        max_points: int = 20,
        verbose: bool = True
    ):
        self.max_frames = max_frames
        self.max_points = max_points
        self.verbose = verbose
        self.is_ready = False
        self.load_error = None
        
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            required_files = [encoder_path, kmeans_path, mapping_path]
            missing_files = [path for path in required_files if not os.path.exists(path)]
            
            if missing_files:
                self.load_error = f"Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…ÙÙ‚ÙˆØ¯Ø©: {missing_files}"
                logger.warning(f"âš ï¸ {self.load_error}")
                return
            
            # Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self.input_dim = max_frames * max_points * 6
            self.latent_dim = 128
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø·Ø±Ù‚ Ù…Ø®ØªÙ„ÙØ©
            self.encoder = self._load_encoder_compatible(encoder_path)
            if self.encoder is None:
                return
                
            self.encoder.eval()
            
            # ØªØ­Ù…ÙŠÙ„ KMeans Ùˆ Mapping
            try:
                self.kmeans = load(kmeans_path)
                self.mapping = load(mapping_path)
            except Exception as e:
                self.load_error = f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ KMeans/Mapping: {e}"
                logger.error(self.load_error)
                return
            
            self.is_ready = True
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
            logger.info(f"ğŸ“Š ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {len(self.mapping)} Ø¹Ù†Ù‚ÙˆØ¯")
            
        except Exception as e:
            self.load_error = f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}"
            logger.error(self.load_error)
            self.is_ready = False

    def _load_encoder_compatible(self, encoder_path):
        """Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„encoder Ø¨Ø·Ø±Ù‚ Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø®ØªÙ„ÙØ©"""
        methods = [
            self._load_with_dynamic_build,
            self._load_with_compatible_encoder, 
            self._load_with_strict_encoder
        ]
        
        for method in methods:
            try:
                encoder = method(encoder_path)
                if encoder is not None:
                    logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: {method.__name__}")
                    return encoder
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„Øª Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© {method.__name__}: {e}")
                continue
        
        self.load_error = "ÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ø·Ø±Ù‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"
        return None

    def _load_with_dynamic_build(self, encoder_path):
        """Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹"""
        encoder = FlexibleEncoder()
        state_dict = torch.load(encoder_path, map_location='cpu')
        encoder.build_from_state_dict(state_dict, self.input_dim)
        encoder.load_state_dict(state_dict, strict=False)
        return encoder

    def _load_with_compatible_encoder(self, encoder_path):
        """Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ BatchNorm"""
        encoder = CompatibleEncoder(self.input_dim, self.latent_dim)
        state_dict = torch.load(encoder_path, map_location='cpu')
        
        # ØªØ­Ù…ÙŠÙ„ Ù…Ø¹ ØªØ¬Ø§Ù‡Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ØºÙŠØ± Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©
        encoder.load_state_dict(state_dict, strict=False)
        return encoder

    def _load_with_strict_encoder(self, encoder_path):
        """Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø³ÙŠØ· Ù…Ø¹ strict=False"""
        # Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ· ÙŠØ´Ø¨Ù‡ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø£ØµÙ„ÙŠØ©
        encoder = torch.nn.Sequential(
            torch.nn.Linear(self.input_dim, 512),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(512),
            torch.nn.Linear(512, 256), 
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(256),
            torch.nn.Linear(256, self.latent_dim)
        )
        
        state_dict = torch.load(encoder_path, map_location='cpu')
        encoder.load_state_dict(state_dict, strict=False)
        return encoder

    # Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø¯ÙˆØ§Ù„ ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ (normalize_gesture, resample_frames, etc.)
    def normalize_gesture(self, frames: List[Dict]) -> List[Dict]:
        """ØªØ·Ø¨ÙŠØ¹ Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø§Ù„Ø¥ÙŠÙ…Ø§Ø¡Ø©"""
        if not frames:
            return frames
            
        # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø· Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª
        all_points = []
        for frame in frames:
            points = frame.get("points", [])
            for point in points:
                all_points.append((point.get("x", 0), point.get("y", 0)))
        
        if not all_points:
            return frames
            
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¯Ù‰
        xs = [p[0] for p in all_points]
        ys = [p[1] for p in all_points]
        
        min_x, max_x = min(xs), max(xs)
        min_y, max_y = min(ys), max(ys)
        
        width = max(max_x - min_x, 1.0)
        height = max(max_y - min_y, 1.0)
        scale = max(width, height, 1.0)
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª
        normalized_frames = []
        for frame in frames:
            normalized_points = []
            points = frame.get("points", [])
            for point in points:
                x_norm = (point.get("x", 0) - min_x) / scale
                y_norm = (point.get("y", 0) - min_y) / scale
                normalized_points.append({
                    "x": float(x_norm),
                    "y": float(y_norm),
                    "pressure": point.get("pressure", 1.0)
                })
            normalized_frames.append({
                "timestamp": frame.get("ts", frame.get("timestamp", 0)),
                "delta_ms": frame.get("delta_ms", 16),
                "points": normalized_points
            })
        
        return normalized_frames

    def resample_frames(self, frames: List[Dict], target_frames: int) -> List[Dict]:
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª"""
        if len(frames) <= 1 or target_frames <= 0:
            return frames
            
        if len(frames) == target_frames:
            return frames
            
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· Ù…Ù† ÙƒÙ„ Ø¥Ø·Ø§Ø±
        frame_points = []
        for frame in frames:
            points = frame.get("points", [])
            frame_points.append(points)
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø®Ø·ÙŠ
        original_indices = np.linspace(0, len(frames) - 1, len(frames))
        target_indices = np.linspace(0, len(frames) - 1, target_frames)
        
        resampled_frames = []
        for target_idx in target_indices:
            idx = int(round(target_idx))
            idx = min(idx, len(frames) - 1)
            
            # Ù†Ø³Ø® Ø§Ù„Ù†Ù‚Ø§Ø·
            copied_points = []
            for point in frame_points[idx]:
                copied_points.append({
                    "x": point.get("x", 0),
                    "y": point.get("y", 0),
                    "pressure": point.get("pressure", 1.0)
                })
            
            resampled_frames.append({
                "timestamp": frames[idx].get("ts", frames[idx].get("timestamp", 0)),
                "delta_ms": frames[idx].get("delta_ms", 16),
                "points": copied_points
            })
        
        return resampled_frames

    def resample_points_per_frame(self, frames: List[Dict], target_points: int) -> List[Dict]:
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· ÙÙŠ ÙƒÙ„ Ø¥Ø·Ø§Ø±"""
        resampled_frames = []
        
        for frame in frames:
            points = frame.get("points", [])
            if len(points) == target_points:
                resampled_frames.append(frame)
                continue
                
            if len(points) <= 1:
                default_point = points[0] if points else {"x": 0, "y": 0, "pressure": 1.0}
                resampled_points = [default_point.copy() for _ in range(target_points)]
                resampled_frames.append({
                    "timestamp": frame.get("timestamp", 0),
                    "delta_ms": frame.get("delta_ms", 16),
                    "points": resampled_points
                })
                continue
            
            xs = [p["x"] for p in points]
            ys = [p["y"] for p in points]
            pressures = [p.get("pressure", 1.0) for p in points]
            
            original_indices = np.linspace(0, len(points)-1, len(points))
            target_indices = np.linspace(0, len(points)-1, target_points)
            
            new_xs = np.interp(target_indices, original_indices, xs)
            new_ys = np.interp(target_indices, original_indices, ys)
            new_pressures = np.interp(target_indices, original_indices, pressures)
            
            resampled_points = []
            for i in range(target_points):
                resampled_points.append({
                    "x": float(new_xs[i]),
                    "y": float(new_ys[i]),
                    "pressure": float(new_pressures[i])
                })
            
            resampled_frames.append({
                "timestamp": frame.get("timestamp", 0),
                "delta_ms": frame.get("delta_ms", 16),
                "points": resampled_points
            })
        
        return resampled_frames

    def calculate_derived_features(self, frames: List[Dict]) -> List[Dict]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø´ØªÙ‚Ø© (dx, dy, angle)"""
        if len(frames) <= 1:
            enhanced_frames = []
            for frame in frames:
                enhanced_points = []
                for point in frame.get("points", []):
                    enhanced_points.append({
                        "x": point.get("x", 0),
                        "y": point.get("y", 0),
                        "pressure": point.get("pressure", 1.0),
                        "dx": 0.0,
                        "dy": 0.0,
                        "angle": 0.0
                    })
                enhanced_frames.append({
                    "timestamp": frame.get("timestamp", 0),
                    "delta_ms": frame.get("delta_ms", 16),
                    "points": enhanced_points
                })
            return enhanced_frames
        
        enhanced_frames = []
        
        first_frame = frames[0]
        first_points = []
        for point in first_frame.get("points", []):
            first_points.append({
                "x": point.get("x", 0),
                "y": point.get("y", 0),
                "pressure": point.get("pressure", 1.0),
                "dx": 0.0,
                "dy": 0.0,
                "angle": 0.0
            })
        enhanced_frames.append({
            "timestamp": first_frame.get("timestamp", 0),
            "delta_ms": first_frame.get("delta_ms", 16),
            "points": first_points
        })
        
        for i in range(1, len(frames)):
            current_frame = frames[i]
            prev_frame = frames[i-1]
            
            enhanced_points = []
            current_points = current_frame.get("points", [])
            prev_points = prev_frame.get("points", [])
            
            min_points = min(len(current_points), len(prev_points))
            
            for j in range(min_points):
                curr_pt = current_points[j]
                prev_pt = prev_points[j]
                
                dx = curr_pt.get("x", 0) - prev_pt.get("x", 0)
                dy = curr_pt.get("y", 0) - prev_pt.get("y", 0)
                angle = np.arctan2(dy, dx) if (dx != 0 or dy != 0) else 0.0
                
                enhanced_points.append({
                    "x": curr_pt.get("x", 0),
                    "y": curr_pt.get("y", 0),
                    "pressure": curr_pt.get("pressure", 1.0),
                    "dx": float(dx),
                    "dy": float(dy),
                    "angle": float(angle)
                })
            
            for j in range(min_points, len(current_points)):
                curr_pt = current_points[j]
                enhanced_points.append({
                    "x": curr_pt.get("x", 0),
                    "y": curr_pt.get("y", 0),
                    "pressure": curr_pt.get("pressure", 1.0),
                    "dx": 0.0,
                    "dy": 0.0,
                    "angle": 0.0
                })
            
            enhanced_frames.append({
                "timestamp": current_frame.get("timestamp", 0),
                "delta_ms": current_frame.get("delta_ms", 16),
                "points": enhanced_points
            })
        
        return enhanced_frames

    def gesture_to_tensor(self, frames: List[Dict]) -> np.ndarray:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥ÙŠÙ…Ø§Ø¡Ø© Ø¥Ù„Ù‰ tensor"""
        F = len(frames)
        P = self.max_points
        C = 6
        
        tensor = np.zeros((F, P, C), dtype=np.float32)
        
        for fi, frame in enumerate(frames):
            points = frame.get("points", [])
            for pi, point in enumerate(points[:P]):
                tensor[fi, pi, 0] = point.get("x", 0.0)
                tensor[fi, pi, 1] = point.get("y", 0.0)
                tensor[fi, pi, 2] = point.get("pressure", 1.0)
                tensor[fi, pi, 3] = point.get("dx", 0.0)
                tensor[fi, pi, 4] = point.get("dy", 0.0)
                tensor[fi, pi, 5] = point.get("angle", 0.0)
        
        return tensor

    def preprocess_gesture(self, gesture_data: Dict[str, Any]) -> np.ndarray:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø© Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©"""
        try:
            frames = gesture_data.get("frames", [])
            
            if not frames:
                raise ValueError("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
            logger.info(f"ğŸ“¥ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥ÙŠÙ…Ø§Ø¡Ø©: {len(frames)} Ø¥Ø·Ø§Ø±")
            
            normalized_frames = self.normalize_gesture(frames)
            resampled_frames = self.resample_frames(normalized_frames, self.max_frames)
            final_frames = self.resample_points_per_frame(resampled_frames, self.max_points)
            enhanced_frames = self.calculate_derived_features(final_frames)
            tensor_data = self.gesture_to_tensor(enhanced_frames)
            
            logger.info(f"âœ… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§ÙƒØªÙ…Ù„Øª: tensor shape {tensor_data.shape}")
            
            return tensor_data
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥ÙŠÙ…Ø§Ø¡Ø©: {e}")
            raise

    def predict_gesture(self, gesture_data: Dict[str, Any]) -> Dict[str, Any]:
        if not self.is_ready:
            return {
                "success": False,
                "error": f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ø¬Ø§Ù‡Ø² Ù„Ù„ØªÙ†Ø¨Ø¤. {self.load_error}",
                "predicted_letter": "?",
                "cluster": None
            }
        
        try:
            logger.info("ğŸ¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤...")
            
            processed_tensor = self.preprocess_gesture(gesture_data)
            flattened = processed_tensor.reshape(1, -1)
            input_tensor = torch.tensor(flattened, dtype=torch.float32)
            
            with torch.no_grad():
                latent = self.encoder(input_tensor).numpy()[0]
            
            cluster_idx = int(self.kmeans.predict([latent])[0])
            predicted_letter = self.mapping.get(cluster_idx, f"Cluster_{cluster_idx}")
            
            logger.info(f"ğŸ‰ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: '{predicted_letter}' (Ø§Ù„Ø¹Ù†Ù‚ÙˆØ¯: {cluster_idx})")
            
            return {
                "success": True,
                "predicted_letter": predicted_letter,
                "cluster": cluster_idx,
                "confidence": 1.0
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            return {
                "success": False,
                "error": str(e),
                "predicted_letter": "?",
                "cluster": None
            }

    def get_status(self) -> Dict[str, Any]:
        status = {
            "is_ready": self.is_ready,
            "model_loaded": self.is_ready,
            "max_frames": self.max_frames,
            "max_points": self.max_points,
            "input_dim": self.input_dim,
            "latent_dim": self.latent_dim
        }
        
        if self.load_error:
            status["load_error"] = self.load_error
            
        if self.is_ready:
            status["clusters_count"] = len(self.mapping)
            status["mapping_sample"] = dict(list(self.mapping.items())[:5])
        
        return status