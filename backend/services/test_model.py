import pickle
import numpy as np
import pandas as pd
import os
from tensorflow.keras.models import load_model
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

def plot_confusion_matrix(y_true, y_pred, classes, filename="confusion_matrix_test.png"):
    """Ø±Ø³Ù… Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª"""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, 
                cmap='Blues', cbar=True, annot_kws={"size": 10})
    plt.xlabel('Predicted', fontsize=12)
    plt.ylabel('Actual', fontsize=12)
    plt.title('Confusion Matrix - Test Set', fontsize=14, pad=20)
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ… Confusion matrix saved as {filename}")

def plot_prediction_distribution(y_true, y_pred, classes, filename="prediction_distribution.png"):
    """Ø±Ø³Ù… ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
    true_counts = [np.sum(y_true == i) for i in range(len(classes))]
    ax1.bar(classes, true_counts, color='skyblue', alpha=0.7, label='True')
    ax1.set_title('True Labels Distribution')
    ax1.set_xlabel('Class')
    ax1.set_ylabel('Count')
    ax1.tick_params(axis='x', rotation=45)
    
    # Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
    pred_counts = [np.sum(y_pred == i) for i in range(len(classes))]
    ax2.bar(classes, pred_counts, color='lightcoral', alpha=0.7, label='Predicted')
    ax2.set_title('Predicted Labels Distribution')
    ax2.set_xlabel('Class')
    ax2.set_ylabel('Count')
    ax2.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ… Prediction distribution saved as {filename}")

def apply_consistent_scaling(X_test, scaler):
    """
    âœ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø¨Ù†ÙØ³ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ù„Ø¶Ø¨Ø·
    Ù‡Ø°Ù‡ Ù†Ø³Ø®Ø© Ø·Ø¨Ù‚ Ø§Ù„Ø£ØµÙ„ Ù…Ù† Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ training_pipeline
    """
    print("\nğŸ”§ Applying consistent scaling (same as training)...")
    
    # Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ training_pipeline
    n_samples, timesteps, n_features = X_test.shape
    
    # reshape Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†ÙØ³ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    X_test_flat = X_test.reshape(-1, n_features)
    
    print(f"ğŸ“Š Before scaling - Shape: {X_test.shape}")
    print(f"ğŸ“Š Before scaling - Range: [{X_test.min():.4f}, {X_test.max():.4f}]")
    print(f"ğŸ“Š Before scaling - Mean: {X_test.mean():.4f}, Std: {X_test.std():.4f}")
    
    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø¨Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©
    X_test_scaled_flat = scaler.transform(X_test_flat)
    X_test_scaled = X_test_scaled_flat.reshape(X_test.shape)
    
    print(f"ğŸ“Š After scaling - Range: [{X_test_scaled.min():.4f}, {X_test_scaled.max():.4f}]")
    print(f"ğŸ“Š After scaling - Mean: {X_test_scaled.mean():.4f}, Std: {X_test_scaled.std():.4f}")
    
    # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹Ù‚ÙˆÙ„Ø©
    if abs(X_test_scaled.mean()) > 5 or X_test_scaled.std() > 5:
        print("âš ï¸  Warning: Scaled data statistics are higher than expected")
        print("ğŸ’¡ This might indicate data distribution shift between train and test")
    
    return X_test_scaled

def analyze_predictions(y_true, y_pred, y_pred_proba, classes):
    """ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
    print("\n" + "="*50)
    print("ğŸ” PREDICTION ANALYSIS")
    print("="*50)
    
    # ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
    unique_pred, pred_counts = np.unique(y_pred, return_counts=True)
    pred_dist = dict(zip(unique_pred, pred_counts))
    
    print(f"ğŸ“Š Prediction distribution: {pred_dist}")
    
    # Ø§ÙƒØªØ´Ø§Ù Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØªÙˆÙ‚Ø¹ Ø£ØµÙ†Ø§ÙØ§Ù‹ Ù…Ø­Ø¯ÙˆØ¯Ø©
    if len(unique_pred) <= 2:
        print("ğŸš¨ CRITICAL: Model is predicting only 1-2 classes!")
        predicted_classes = [classes[i] for i in unique_pred]
        print(f"   Predicted classes: {predicted_classes}")
    elif len(unique_pred) < len(classes) // 2:
        print("âš ï¸  WARNING: Model is predicting only few classes")
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
    confidence_scores = np.max(y_pred_proba, axis=1)
    avg_confidence = np.mean(confidence_scores)
    low_confidence_threshold = 0.6
    low_confidence_count = np.sum(confidence_scores < low_confidence_threshold)
    
    print(f"ğŸ“ˆ Average prediction confidence: {avg_confidence:.3f}")
    print(f"ğŸ“‰ Samples with confidence < {low_confidence_threshold}: {low_confidence_count}/{len(y_pred)} ({low_confidence_count/len(y_pred)*100:.1f}%)")
    
    # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ ØµÙ†Ù
    print("\nğŸ“‹ Per-class analysis:")
    class_results = []
    for i, class_name in enumerate(classes):
        class_mask = y_true == i
        if np.sum(class_mask) > 0:
            class_accuracy = np.mean(y_pred[class_mask] == i)
            class_confidence = np.mean(confidence_scores[class_mask])
            class_samples = np.sum(class_mask)
            class_results.append({
                'class': class_name,
                'accuracy': class_accuracy,
                'confidence': class_confidence,
                'samples': class_samples
            })
            print(f"   {class_name}: Accuracy={class_accuracy:.3f}, Confidence={class_confidence:.3f}, Samples={class_samples}")
    
    return class_results

def save_detailed_results(y_true, y_pred, y_pred_proba, classes, filename="test_results_detailed.csv"):
    """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ÙØµÙ„Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„"""
    results = []
    for i, (true, pred, proba) in enumerate(zip(y_true, y_pred, y_pred_proba)):
        results.append({
            'sample_id': i,
            'true_label': classes[true],
            'true_label_idx': true,
            'predicted_label': classes[pred],
            'predicted_label_idx': pred,
            'confidence': proba[pred],
            'is_correct': true == pred,
            'max_probability': np.max(proba),
            'entropy': -np.sum(proba * np.log(proba + 1e-8)),  # Ù‚ÙŠØ§Ø³ Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ†
            **{f'prob_{cls}': prob for cls, prob in zip(classes, proba)}
        })
    
    df = pd.DataFrame(results)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"âœ… Detailed results saved to {filename}")
    
    # Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    accuracy = accuracy_score(y_true, y_pred)
    print(f"\nğŸ“ˆ Overall Accuracy: {accuracy:.3f}")
    print(f"ğŸ“Š Correct predictions: {df['is_correct'].sum()}/{len(df)}")
    
    return df

def check_model_compatibility(model, X_test_scaled, label_encoder):
    """ÙØ­Øµ ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    print("\nğŸ” Checking model compatibility...")
    
    # ÙØ­Øµ Ø´ÙƒÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
    expected_input_shape = model.input_shape[1:]  # (150, 21)
    actual_input_shape = X_test_scaled.shape[1:]  # (150, 21)
    
    print(f"ğŸ“ Model expects input shape: {expected_input_shape}")
    print(f"ğŸ“ Actual test data shape: {actual_input_shape}")
    
    if expected_input_shape != actual_input_shape:
        print(f"âŒ Shape mismatch! Model: {expected_input_shape}, Data: {actual_input_shape}")
        return False
    
    # ÙØ­Øµ Ø¹Ø¯Ø¯ Ø§Ù„Ø£ØµÙ†Ø§Ù
    expected_output_classes = model.output_shape[-1]
    actual_classes = len(label_encoder.classes_)
    
    print(f"ğŸ¯ Model expects {expected_output_classes} output classes")
    print(f"ğŸ¯ Actual number of classes: {actual_classes}")
    
    if expected_output_classes != actual_classes:
        print(f"âŒ Class count mismatch! Model: {expected_output_classes}, Data: {actual_classes}")
        return False
    
    print("âœ… Model and data are compatible!")
    return True

def main():
    print("ğŸš€ Starting Enhanced Model Testing with Consistent Scaling...")
    print("="*60)
    
    try:
        # 1ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        print("ğŸ“¥ Loading test data...")
        try:
            with open("X_test_fixed.pkl", "rb") as f:
                X_test = pickle.load(f)
            print("âœ… Using fixed test data")
        except:
            with open("X_test.pkl", "rb") as f:
                X_test = pickle.load(f)
            print("âš ï¸  Using original test data")
        with open("y_test.pkl", "rb") as f:
            y_test = pickle.load(f)
        print(f"âœ… Loaded X_test: {X_test.shape}, y_test: {y_test.shape}")

        # 2ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Scaler Ùˆ LabelEncoder
        print("ğŸ“¥ Loading preprocessing objects...")
        with open("scaler.pkl", "rb") as f:
            scaler = pickle.load(f)
        with open("label_encoder.pkl", "rb") as f:
            label_encoder = pickle.load(f)
        print(f"âœ… Loaded scaler and label encoder")
        print(f"ğŸ”  Classes: {label_encoder.classes_}")

        # 3ï¸âƒ£ âœ… ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø¨Ù†ÙØ³ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ù„Ø¶Ø¨Ø·
        X_test_scaled = apply_consistent_scaling(X_test, scaler)

        # 4ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
        print("\nğŸ“¥ Loading trained model...")
        model_files = [
            "arabic_gesture_cnn_best.h5",
            "arabic_gesture_cnn_final.h5", 
            "model.h5"
        ]
        
        model = None
        for model_file in model_files:
            if os.path.exists(model_file):
                model = load_model(model_file)
                print(f"âœ… Loaded model: {model_file}")
                break
        
        if model is None:
            raise FileNotFoundError("âŒ No model file found! Please train the model first.")

        # 5ï¸âƒ£ âœ… ÙØ­Øµ ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if not check_model_compatibility(model, X_test_scaled, label_encoder):
            print("âŒ Model and data are incompatible. Please retrain the model.")
            return

        # 6ï¸âƒ£ Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ø¹ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        print("\nğŸ¯ Making predictions...")
        y_pred_proba = model.predict(X_test_scaled, verbose=1)
        y_pred = np.argmax(y_pred_proba, axis=1)
        print(f"âœ… Predictions completed. Shape: {y_pred.shape}")

        # 7ï¸âƒ£ ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        class_results = analyze_predictions(y_test, y_pred, y_pred_proba, label_encoder.classes_)

        # 8ï¸âƒ£ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ÙØµÙ„
        print("\n" + "="*50)
        print("ğŸ“Š DETAILED CLASSIFICATION REPORT")
        print("="*50)
        print(classification_report(y_test, y_pred, 
                                  target_names=label_encoder.classes_, 
                                  zero_division=0,
                                  digits=3))

        # 9ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ÙØµÙ„Ø©
        results_df = save_detailed_results(y_test, y_pred, y_pred_proba, label_encoder.classes_)

        # ğŸ”Ÿ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
        plot_confusion_matrix(y_test, y_pred, label_encoder.classes_)
        plot_prediction_distribution(y_test, y_pred, label_encoder.classes_)

        # ğŸ”„ ØªØ­Ù„ÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠ Ù„Ù„Ø£ØµÙ†Ø§Ù Ø°Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¶Ø¹ÙŠÙ
        poor_performers = [cr for cr in class_results if cr['accuracy'] < 0.5]
        if poor_performers:
            print("\nâš ï¸  POOR PERFORMING CLASSES (Accuracy < 50%):")
            for cr in poor_performers:
                print(f"   {cr['class']}: {cr['accuracy']:.1%} accuracy")

        # ğŸ‰ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print("\n" + "="*50)
        print("ğŸ‰ TESTING COMPLETED SUCCESSFULLY!")
        print("="*50)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"ğŸ† Final Test Accuracy: {accuracy:.3f}")
        
        # ØªÙ‚Ø¯ÙŠØ± Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø©
        if accuracy >= 0.9:
            print("ğŸŒŸ EXCELLENT: Model is ready for deployment!")
        elif accuracy >= 0.8:
            print("âœ… VERY GOOD: Model performance is strong")
        elif accuracy >= 0.7:
            print("ğŸ“— GOOD: Model is acceptable but can be improved") 
        elif accuracy >= 0.6:
            print("ğŸ“™ FAIR: Model needs improvement")
        else:
            print("ğŸ“• POOR: Model requires significant improvements")
            
        print(f"\nğŸ“ Results saved in:")
        print(f"   - test_results_detailed.csv")
        print(f"   - confusion_matrix_test.png") 
        print(f"   - prediction_distribution.png")

    except Exception as e:
        print(f"âŒ Error during testing: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

# import pickle
# import numpy as np
# import pandas as pd
# from tensorflow.keras.models import load_model
# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
# import matplotlib.pyplot as plt
# import seaborn as sns
# import os

# def plot_confusion_matrix(y_true, y_pred, classes, filename="confusion_matrix_test.png"):
#     """Ø±Ø³Ù… Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª"""
#     cm = confusion_matrix(y_true, y_pred)
#     plt.figure(figsize=(10, 8))
#     sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, 
#                 cmap='Blues', cbar=True, annot_kws={"size": 12})
#     plt.xlabel('Predicted', fontsize=12)
#     plt.ylabel('Actual', fontsize=12)
#     plt.title('Confusion Matrix - Test Set', fontsize=14, pad=20)
#     plt.xticks(rotation=45)
#     plt.yticks(rotation=0)
#     plt.tight_layout()
#     plt.savefig(filename, dpi=300, bbox_inches='tight')
#     plt.show()
#     print(f"âœ… Confusion matrix saved as {filename}")

# def plot_prediction_distribution(y_true, y_pred, classes, filename="prediction_distribution.png"):
#     """Ø±Ø³Ù… ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©"""
#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
#     # Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
#     true_counts = [np.sum(y_true == i) for i in range(len(classes))]
#     ax1.bar(classes, true_counts, color='skyblue', alpha=0.7, label='True')
#     ax1.set_title('True Labels Distribution')
#     ax1.set_xlabel('Class')
#     ax1.set_ylabel('Count')
#     ax1.tick_params(axis='x', rotation=45)
    
#     # Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
#     pred_counts = [np.sum(y_pred == i) for i in range(len(classes))]
#     ax2.bar(classes, pred_counts, color='lightcoral', alpha=0.7, label='Predicted')
#     ax2.set_title('Predicted Labels Distribution')
#     ax2.set_xlabel('Class')
#     ax2.set_ylabel('Count')
#     ax2.tick_params(axis='x', rotation=45)
    
#     plt.tight_layout()
#     plt.savefig(filename, dpi=300, bbox_inches='tight')
#     plt.show()
#     print(f"âœ… Prediction distribution saved as {filename}")

# def analyze_predictions(y_true, y_pred, y_pred_proba, classes):
#     """ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„ØªÙ†Ø¨Ø¤Ø§Øª"""
#     print("\n" + "="*50)
#     print("ğŸ” PREDICTION ANALYSIS")
#     print("="*50)
    
#     # ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
#     unique_pred, pred_counts = np.unique(y_pred, return_counts=True)
#     pred_dist = dict(zip(unique_pred, pred_counts))
    
#     print(f"ğŸ“Š Prediction distribution: {pred_dist}")
    
#     # Ø§ÙƒØªØ´Ø§Ù Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØªÙˆÙ‚Ø¹ ØµÙ†ÙØ§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ ÙÙ‚Ø·
#     if len(unique_pred) == 1:
#         print("ğŸš¨ CRITICAL: Model is predicting only ONE class!")
#         print(f"   All predictions are for class: {classes[unique_pred[0]]}")
#     elif len(unique_pred) < len(classes) // 2:
#         print("âš ï¸  WARNING: Model is predicting only few classes")
    
#     # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
#     confidence_scores = np.max(y_pred_proba, axis=1)
#     avg_confidence = np.mean(confidence_scores)
#     low_confidence_threshold = 0.6
#     low_confidence_count = np.sum(confidence_scores < low_confidence_threshold)
    
#     print(f"ğŸ“ˆ Average prediction confidence: {avg_confidence:.3f}")
#     print(f"ğŸ“‰ Samples with confidence < {low_confidence_threshold}: {low_confidence_count}/{len(y_pred)} ({low_confidence_count/len(y_pred)*100:.1f}%)")
    
#     # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ ØµÙ†Ù
#     print("\nğŸ“‹ Per-class analysis:")
#     for i, class_name in enumerate(classes):
#         class_mask = y_true == i
#         if np.sum(class_mask) > 0:
#             class_accuracy = np.mean(y_pred[class_mask] == i)
#             class_confidence = np.mean(confidence_scores[class_mask])
#             print(f"   {class_name}: Accuracy={class_accuracy:.3f}, Avg Confidence={class_confidence:.3f}")

# def validate_data_quality(X_test, y_test, scaler):
#     """ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
#     print("\n" + "="*50)
#     print("ğŸ” DATA QUALITY CHECK")
#     print("="*50)
    
#     # ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©
#     print(f"ğŸ“Š X_test shape: {X_test.shape}")
#     print(f"ğŸ“Š y_test shape: {y_test.shape}")
#     print(f"ğŸ¯ Unique classes in y_test: {np.unique(y_test)}")
#     print(f"ğŸ“ˆ Class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}")
    
#     # ÙØ­Øµ Ø§Ù„Ù‚ÙŠÙ… ØºÙŠØ± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
#     print(f"ğŸ” Data stats - Min: {X_test.min():.4f}, Max: {X_test.max():.4f}")
#     print(f"ğŸ” Data stats - Mean: {X_test.mean():.4f}, Std: {X_test.std():.4f}")
#     print(f"ğŸ” NaN values: {np.isnan(X_test).sum()}, Inf values: {np.isinf(X_test).sum()}")
    
#     # ÙØ­Øµ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
#     X_test_flat = X_test.reshape(-1, X_test.shape[-1])
#     X_test_scaled_flat = scaler.transform(X_test_flat)
    
#     print(f"ğŸ“Š After scaling - Min: {X_test_scaled_flat.min():.4f}, Max: {X_test_scaled_flat.max():.4f}")
#     print(f"ğŸ“Š After scaling - Mean: {X_test_scaled_flat.mean():.4f}, Std: {X_test_scaled_flat.std():.4f}")
    
#     # ØªØ­Ø°ÙŠØ± Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠØ©
#     if abs(X_test_scaled_flat.mean()) > 10 or X_test_scaled_flat.std() > 10:
#         print("âš ï¸  WARNING: Scaled data has unusual statistics!")
    
#     return X_test_scaled_flat.reshape(X_test.shape)

# def save_detailed_results(y_true, y_pred, y_pred_proba, classes, filename="test_results_detailed.csv"):
#     """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ÙØµÙ„Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„"""
#     results = []
#     for i, (true, pred, proba) in enumerate(zip(y_true, y_pred, y_pred_proba)):
#         results.append({
#             'sample_id': i,
#             'true_label': classes[true],
#             'true_label_idx': true,
#             'predicted_label': classes[pred],
#             'predicted_label_idx': pred,
#             'confidence': proba[pred],
#             'is_correct': true == pred,
#             'max_probability': np.max(proba),
#             'entropy': -np.sum(proba * np.log(proba + 1e-8))  # Ù‚ÙŠØ§Ø³ Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ†
#         })
    
#     df = pd.DataFrame(results)
#     df.to_csv(filename, index=False, encoding='utf-8-sig')
#     print(f"âœ… Detailed results saved to {filename}")
    
#     # Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
#     accuracy = accuracy_score(y_true, y_pred)
#     print(f"\nğŸ“ˆ Overall Accuracy: {accuracy:.3f}")
#     print(f"ğŸ“Š Correct predictions: {df['is_correct'].sum()}/{len(df)}")
    
#     return df

# def main():
#     print("ğŸš€ Starting Enhanced Model Testing...")
#     print("="*60)
    
#     try:
#         # 1ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
#         print("ğŸ“¥ Loading test data...")
#         with open("X_test.pkl", "rb") as f:
#             X_test = pickle.load(f)
#         with open("y_test.pkl", "rb") as f:
#             y_test = pickle.load(f)
#         print(f"âœ… Loaded X_test: {X_test.shape}, y_test: {y_test.shape}")

#         # 2ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Scaler Ùˆ LabelEncoder
#         print("ğŸ“¥ Loading preprocessing objects...")
#         with open("scaler.pkl", "rb") as f:
#             scaler = pickle.load(f)
#         with open("label_encoder.pkl", "rb") as f:
#             label_encoder = pickle.load(f)
#         print(f"âœ… Loaded scaler and label encoder")
#         print(f"ğŸ”  Classes: {label_encoder.classes_}")

#         # 3ï¸âƒ£ ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
#         X_test_scaled = validate_data_quality(X_test, y_test, scaler)

#         # 4ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
#         print("\nğŸ“¥ Loading trained model...")
#         if os.path.exists("arabic_gesture_cnn_best.h5"):
#             model = load_model("arabic_gesture_cnn_best.h5")
#             print("âœ… Loaded model: arabic_gesture_cnn_best.h5")
#         elif os.path.exists("arabic_gesture_cnn_final.h5"):
#             model = load_model("arabic_gesture_cnn_final.h5")
#             print("âœ… Loaded model: arabic_gesture_cnn_final.h5")
#         else:
#             raise FileNotFoundError("âŒ No model file found!")

#         # Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
#         print(f"âœ… Model input shape: {model.input_shape}")
#         print(f"âœ… Model output shape: {model.output_shape}")

#         # 5ï¸âƒ£ Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ø¹ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
#         print("\nğŸ¯ Making predictions...")
#         y_pred_proba = model.predict(X_test_scaled, verbose=1)
#         y_pred = np.argmax(y_pred_proba, axis=1)
#         print(f"âœ… Predictions completed. Shape: {y_pred.shape}")

#         # 6ï¸âƒ£ ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„ØªÙ†Ø¨Ø¤Ø§Øª
#         analyze_predictions(y_test, y_pred, y_pred_proba, label_encoder.classes_)

#         # 7ï¸âƒ£ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ÙØµÙ„
#         print("\n" + "="*50)
#         print("ğŸ“Š DETAILED CLASSIFICATION REPORT")
#         print("="*50)
#         print(classification_report(y_test, y_pred, 
#                                   target_names=label_encoder.classes_, 
#                                   zero_division=0,
#                                   digits=3))

#         # 8ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ÙØµÙ„Ø©
#         results_df = save_detailed_results(y_test, y_pred, y_pred_proba, label_encoder.classes_)

#         # 9ï¸âƒ£ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
#         plot_confusion_matrix(y_test, y_pred, label_encoder.classes_)
#         plot_prediction_distribution(y_test, y_pred, label_encoder.classes_)

#         # ğŸ”Ÿ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
#         print("\n" + "="*50)
#         print("ğŸ‰ TESTING COMPLETED SUCCESSFULLY!")
#         print("="*50)
#         accuracy = accuracy_score(y_test, y_pred)
#         print(f"ğŸ† Final Test Accuracy: {accuracy:.3f}")
#         print(f"ğŸ“ Results saved in:")
#         print(f"   - test_results_detailed.csv")
#         print(f"   - confusion_matrix_test.png") 
#         print(f"   - prediction_distribution.png")
        
#         # Ù†ØµÙŠØ­Ø© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
#         if accuracy < 0.5:
#             print("\nğŸ’¡ RECOMMENDATION: Model performance is low. Consider:")
#             print("   - Retraining with better data preprocessing")
#             print("   - Checking for class imbalance")
#             print("   - Verifying feature extraction")
#         elif accuracy < 0.8:
#             print("\nğŸ’¡ RECOMMENDATION: Good performance. Can be improved with:")
#             print("   - More training data")
#             print("   - Hyperparameter tuning")
#             print("   - Data augmentation")
#         else:
#             print("\nğŸ’¡ RECOMMENDATION: Excellent performance! Model is ready for deployment.")

#     except Exception as e:
#         print(f"âŒ Error during testing: {e}")
#         import traceback
#         traceback.print_exc()

# if __name__ == "__main__":
#     main()