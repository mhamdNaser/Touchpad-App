# app/services/training_pipeline.py
import json
import pickle
import numpy as np
import matplotlib.pyplot as plt
from typing import List
from app.services.gesture_data_loader import GestureDataLoader
from app.services.features_visualizer import FeatureEngineerVisualizer

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import seaborn as sns

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv1D, MaxPooling1D, GlobalAveragePooling1D,
    Dense, Dropout, BatchNormalization
)
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam


class TrainingPipeline:
    def __init__(self, max_timesteps: int = 150):
        self.data_loader = GestureDataLoader(api_url="https://api.sydev.site/api/gestures")
        self.feature_engineer = FeatureEngineerVisualizer(max_timesteps=max_timesteps)

    # ======================================================
    # âœ… Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ 1D-CNN Ù…Ø­Ø³Ù‘Ù†
    # ======================================================
    def build_cnn_model(self, input_shape, num_classes):
        model = Sequential([
            Conv1D(128, kernel_size=5, activation='relu', padding='same', input_shape=input_shape),
            BatchNormalization(),
            MaxPooling1D(pool_size=2),
            Dropout(0.3),

            Conv1D(64, kernel_size=3, activation='relu', padding='same'),
            BatchNormalization(),
            MaxPooling1D(pool_size=2),
            Dropout(0.3),

            Conv1D(32, kernel_size=3, activation='relu', padding='same'),
            BatchNormalization(),
            GlobalAveragePooling1D(),

            Dense(64, activation='relu'),
            Dropout(0.4),
            Dense(num_classes, activation='softmax')
        ])

        model.compile(
            loss='categorical_crossentropy',
            optimizer=Adam(learning_rate=0.0005),
            metrics=['accuracy']
        )
        
        print(f"âœ… Built CNN model with input shape {input_shape} and {num_classes} classes")
        return model

    # ======================================================
    # ğŸ”§ ØªØ­ÙˆÙŠÙ„ Ù…ÙØ§ØªÙŠØ­ dict Ø¥Ù„Ù‰ int
    # ======================================================
    def _convert_keys_to_int(self, d):
        result = {}
        for k, v in d.items():
            key = int(k) if isinstance(k, (np.integer, np.int64)) else k
            if isinstance(v, dict):
                result[key] = self._convert_keys_to_int(v)
            elif isinstance(v, (np.integer, np.int64)):
                result[key] = int(v)
            else:
                result[key] = v
        return result

    # ======================================================
    # ğŸ“Š Ø±Ø³Ù… Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³
    # ======================================================
    def plot_confusion_matrix(self, y_true, y_pred, classes):
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(12, 8))
        sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title('Confusion Matrix')
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()

    # ======================================================
    # ğŸ“ˆ Ø±Ø³Ù… Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    # ======================================================
    def plot_training_history(self, history):
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Ø±Ø³Ù… Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚
        ax1.plot(history.history['accuracy'], label='Training Accuracy')
        ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')
        ax1.set_title('Model Accuracy')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.legend()
        ax1.grid(True)
        
        # Ø±Ø³Ù… ÙÙ‚Ø¯Ø§Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚
        ax2.plot(history.history['loss'], label='Training Loss')
        ax2.plot(history.history['val_loss'], label='Validation Loss')
        ax2.set_title('Model Loss')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()

    # ======================================================
    # ğŸ” ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    # ======================================================
    def _validate_data_quality(self, X_train, X_val, X_test, y_train, y_val, y_test):
        """ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        print("\nğŸ” Validating data quality...")
        
        # ÙØ­Øµ Ø§Ù„Ù‚ÙŠÙ… ØºÙŠØ± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
        def check_data_stats(data, name):
            flat_data = data.reshape(-1, data.shape[-1])
            print(f"ğŸ“Š {name} - Shape: {data.shape}")
            print(f"   Range: [{data.min():.4f}, {data.max():.4f}]")
            print(f"   Mean: {data.mean():.4f}, Std: {data.std():.4f}")
            print(f"   NaN: {np.isnan(data).sum()}, Inf: {np.isinf(data).sum()}")
            
            # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ„Ù‡Ø§ Ø£ØµÙØ§Ø±
            zero_ratio = (np.abs(flat_data) < 1e-6).mean()
            print(f"   Zero ratio: {zero_ratio:.4f}")
            
            return zero_ratio

        zero_ratios = []
        zero_ratios.append(check_data_stats(X_train, "X_train"))
        zero_ratios.append(check_data_stats(X_val, "X_val")) 
        zero_ratios.append(check_data_stats(X_test, "X_test"))

        # ÙØ­Øµ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        print(f"ğŸ¯ Label distribution:")
        print(f"   Train: {dict(zip(*np.unique(y_train, return_counts=True)))}")
        print(f"   Val: {dict(zip(*np.unique(y_val, return_counts=True)))}")
        print(f"   Test: {dict(zip(*np.unique(y_test, return_counts=True)))}")

        # ØªØ­Ø°ÙŠØ± Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù†Ø³Ø¨Ø© Ø§Ù„Ø£ØµÙØ§Ø± Ø¹Ø§Ù„ÙŠØ©
        if any(ratio > 0.8 for ratio in zero_ratios):
            print("âš ï¸  WARNING: High zero ratio detected - data might be over-padded")

        # ØªØ­Ø°ÙŠØ± Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙØ¦Ø§Øª ØºÙŠØ± Ù…ØªÙˆØ§Ø²Ù†Ø©
        train_counts = np.bincount(y_train)
        if len(train_counts) > 0 and np.std(train_counts) > np.mean(train_counts) * 0.5:
            print("âš ï¸  WARNING: Class imbalance detected")

    # ======================================================
    # ğŸ“Š ØªØµÙˆØ± ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    # ======================================================
    def visualize_feature_means(self, gestures_data):
        print("ğŸ“Š Visualizing feature distributions before training...")
        self.feature_engineer.plot_feature_distribution(gestures_data)

    # ======================================================
    # ğŸš€ ØªÙ†ÙÙŠØ° Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ØªØ¯Ø±ÙŠØ¨ 1D-CNN - Ù…ÙØ­Ø³Ù‘Ù†
    # ======================================================
    def train_model(self):
        print("ğŸš€ Starting 1D-CNN training pipeline...")

        # 1ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        gestures_data = self.data_loader.load_all_gestures()
        print(f"âœ… Loaded {len(gestures_data)} gestures")
        if len(gestures_data) == 0:
            raise ValueError("âŒ No gestures loaded from API.")

        # 1.5ï¸âƒ£ ØªØµÙˆØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.visualize_feature_means(gestures_data)

        # 2ï¸âƒ£ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙˆØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_train, X_val, X_test, y_train, y_val, y_test, split_info, fixed_indices = self.feature_engineer.split_data(gestures_data)
        print("ğŸ“‹ Split information:")
        print(json.dumps(self._convert_keys_to_int(split_info), indent=2, ensure_ascii=False))

        # âœ… ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±
        self._validate_data_quality(X_train, X_val, X_test, y_train, y_val, y_test)

        # Ø­ÙØ¸ fixed_indices
        with open("split_indices.pkl", "wb") as f:
            pickle.dump(fixed_indices, f)

        # 3ï¸âƒ£ âœ… ØªØ­Ø³ÙŠÙ† ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø¥Ù„Ù‰ one-hot
        num_classes = len(self.feature_engineer.label_encoder.classes_)
        print(f"ğŸ¯ Number of classes: {num_classes}")
        print(f"ğŸ”  Class names: {self.feature_engineer.label_encoder.classes_}")

        # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù€ encoder
        for y_set, name in [(y_train, 'train'), (y_val, 'val'), (y_test, 'test')]:
            unique_labels = np.unique(y_set)
            print(f"ğŸ“ {name} set unique labels: {unique_labels}")
            for label in unique_labels:
                if label not in self.feature_engineer.label_encoder.classes_:
                    print(f"âš ï¸  WARNING: Label {label} not in encoder classes!")

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ one-hot encoding
        y_train_cat = to_categorical(y_train, num_classes=num_classes)
        y_val_cat = to_categorical(y_val, num_classes=num_classes)
        y_test_cat = to_categorical(y_test, num_classes=num_classes)

        print(f"âœ… One-hot shapes - Train: {y_train_cat.shape}, Val: {y_val_cat.shape}, Test: {y_test_cat.shape}")

        # 3.5ï¸âƒ£ Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª
        class_weights = compute_class_weight(
            'balanced', 
            classes=np.unique(y_train), 
            y=y_train
        )
        class_weights = dict(enumerate(class_weights))
        print(f"âš–ï¸ Class weights: {class_weights}")

        # 4ï¸âƒ£ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        input_shape = (X_train.shape[1], X_train.shape[2])
        model = self.build_cnn_model(input_shape, num_classes)
        
        # Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model.summary()

        # 5ï¸âƒ£ Callbacks Ù…Ø­Ø³Ù†Ø©
        early_stop = EarlyStopping(
            monitor='val_loss', 
            patience=20,  # âœ… Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØµØ¨Ø±
            restore_best_weights=True,
            verbose=1
        )
        checkpoint = ModelCheckpoint(
            'arabic_gesture_cnn_best.h5', 
            monitor='val_accuracy', 
            save_best_only=True, 
            verbose=1
        )
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss', 
            factor=0.5, 
            patience=10,  # âœ… Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØµØ¨Ø±
            min_lr=1e-7,  # âœ… ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰
            verbose=1
        )

        # 6ï¸âƒ£ âœ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ batch size Ù…Ø­Ø³Ù‘Ù†
        batch_size = min(32, len(X_train) // 4)  # âœ… batch size Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
        if batch_size < 8:
            batch_size = 8
        print(f"ğŸ¯ Using batch size: {batch_size}")

        print("ğŸ‹ï¸ Starting training...")
        history = model.fit(
            X_train, y_train_cat,
            validation_data=(X_val, y_val_cat),
            epochs=100,
            batch_size=batch_size,
            callbacks=[early_stop, checkpoint, reduce_lr],
            class_weight=class_weights,
            verbose=1
        )

        # 7ï¸âƒ£ âœ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
        print("ğŸ“Š Evaluating model...")
        test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)
        print(f"âœ… Test accuracy: {test_acc:.3f}")
        print(f"âœ… Test loss: {test_loss:.3f}")

        # Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        y_pred_proba = model.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1)

        # âœ… ÙØ­Øµ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        unique_pred, pred_counts = np.unique(y_pred, return_counts=True)
        print(f"ğŸ“Š Prediction distribution: {dict(zip(unique_pred, pred_counts))}")

        print("\nğŸ“Š Classification report:")
        print(classification_report(y_test, y_pred, 
                                  target_names=self.feature_engineer.label_encoder.classes_,
                                  zero_division=0))

        # Ø±Ø³Ù… Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³
        self.plot_confusion_matrix(y_test, y_pred, self.feature_engineer.label_encoder.classes_)

        # Ø±Ø³Ù… Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        self.plot_training_history(history)

        # 8ï¸âƒ£ âœ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        model.save("arabic_gesture_cnn_final.h5", save_format='h5')
        with open("scaler.pkl", "wb") as f:
            pickle.dump(self.feature_engineer.scaler, f)
        with open("label_encoder.pkl", "wb") as f:
            pickle.dump(self.feature_engineer.label_encoder, f)

        print("ğŸ’¾ Saved: model, scaler, and label encoder")

        # 9ï¸âƒ£ âœ… Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        with open("X_test.pkl", "wb") as f:
            pickle.dump(X_test, f)
        with open("y_test.pkl", "wb") as f:
            pickle.dump(y_test, f)

        print("ğŸ’¾ Saved test data for future evaluation")

        return {
            'model': model,
            'history': history,
            'test_accuracy': float(test_acc),
            'test_loss': float(test_loss),
            'split_info': self._convert_keys_to_int(split_info),
            'predictions': {
                'y_true': y_test,
                'y_pred': y_pred,
                'y_pred_proba': y_pred_proba
            }
        }

    # ======================================================
    # ğŸ”„ Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ù…Ø¹Ø§ÙŠØ±Ø© Ù…Ø­Ø³Ù†Ø©
    # ======================================================
    def retrain_with_fixed_scaling(self, gestures_data=None):
        """Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¥ØµÙ„Ø§Ø­ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©"""
        print("ğŸ”„ Retraining with fixed scaling...")
        
        if gestures_data is None:
            gestures_data = self.data_loader.load_all_gestures()
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù…Ù† FeatureEngineerVisualizer
        X_train, X_val, X_test, y_train, y_val, y_test, split_info, fixed_indices = self.feature_engineer.split_data(gestures_data)
        
        # Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ
        return self.train_model()