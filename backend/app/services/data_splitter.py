# app/services/data_splitter.py
from sklearn.model_selection import train_test_split
import numpy as np
from typing import Tuple, Dict

class DataSplitter:
    def __init__(self, test_size: float = 0.2, validation_size: float = 0.1, random_state: int = 42):
        """
        test_size: Ù†Ø³Ø¨Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©
        validation_size: Ù†Ø³Ø¨Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ø¨Ø¹Ø¯ Ø®ØµÙ… Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        random_state: Ù„ØªØ«Ø¨ÙŠØª Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ
        """
        self.test_size = test_size
        self.validation_size = validation_size
        self.random_state = random_state
    
    def split_data(self, features: np.ndarray, labels: np.ndarray) -> Tuple:
        """
        ØªÙ‚Ø³ÙŠÙ… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥ÙŠÙ…Ø§Ø¡Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ØŒ ØªØ­Ù‚Ù‚ØŒ ÙˆØ§Ø®ØªØ¨Ø§Ø± (Ø¬Ø§Ù‡Ø²Ø© Ù„Ù€ LSTM)
        """
        if len(features) != len(labels):
            raise ValueError("Features and labels must have the same number of samples")

        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø£ÙƒØ«Ø± Ù…Ù† ÙØ¦Ø© ÙˆØ§Ø­Ø¯Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… stratify
        stratify_labels = labels if len(np.unique(labels)) > 1 else None

        # ØªÙ‚Ø³ÙŠÙ… Ø£ÙˆÙ„ÙŠ: (ØªØ¯Ø±ÙŠØ¨+ØªØ­Ù‚Ù‚) Ùˆ (Ø§Ø®ØªØ¨Ø§Ø±)
        X_temp, X_test, y_temp, y_test = train_test_split(
            features, labels,
            test_size=self.test_size,
            stratify=stratify_labels,
            random_state=self.random_state
        )

        # Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        val_size_adjusted = self.validation_size / (1 - self.test_size)

        stratify_temp = y_temp if len(np.unique(y_temp)) > 1 else None
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp,
            test_size=val_size_adjusted,
            stratify=stratify_temp,
            random_state=self.random_state
        )

        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def get_split_info(self, y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray) -> Dict:
        """
        Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ù† ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø³ÙŠÙ…
        """
        def get_distribution(y):
            unique, counts = np.unique(y, return_counts=True)
            return dict(zip(unique, counts))
        
        info = {
            'train_samples': len(y_train),
            'val_samples': len(y_val),
            'test_samples': len(y_test),
            'train_distribution': get_distribution(y_train),
            'val_distribution': get_distribution(y_val),
            'test_distribution': get_distribution(y_test),
        }

        # Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙØµÙ„Ø©
        print("\nðŸ“Š Data split info:")
        print(f" - Train: {info['train_samples']} samples, distribution: {info['train_distribution']}")
        print(f" - Validation: {info['val_samples']} samples, distribution: {info['val_distribution']}")
        print(f" - Test: {info['test_samples']} samples, distribution: {info['test_distribution']}\n")

        return info



# # app/services/data_splitter.py
# from sklearn.model_selection import train_test_split
# import numpy as np
# from typing import Tuple, Dict

# class DataSplitter:
#     def __init__(self, test_size: float = 0.2, validation_size: float = 0.1, random_state: int = 42):
#         self.test_size = test_size
#         self.validation_size = validation_size
#         self.random_state = random_state
    
#     def split_data(self, features: np.ndarray, labels: np.ndarray) -> Tuple:
#         """
#         ØªÙ‚Ø³ÙŠÙ… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥ÙŠÙ…Ø§Ø¡Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ØŒ ØªØ­Ù‚Ù‚ØŒ ÙˆØ§Ø®ØªØ¨Ø§Ø± (Ø¬Ø§Ù‡Ø²Ø© Ù„Ù€ LSTM)
#         """
#         # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† stratify Ù…Ù…ÙƒÙ†
#         stratify_labels = labels if len(np.unique(labels)) > 1 else None

#         # ØªÙ‚Ø³ÙŠÙ… Ù…Ø¤Ù‚Øª (ØªØ¯Ø±ÙŠØ¨ + ØªØ­Ù‚Ù‚) Ùˆ (Ø§Ø®ØªØ¨Ø§Ø±)
#         X_temp, X_test, y_temp, y_test = train_test_split(
#             features, labels, 
#             test_size=self.test_size,
#             stratify=stratify_labels,
#             random_state=self.random_state
#         )
        
#         # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¹Ø¯Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚
#         val_size_adjusted = self.validation_size / (1 - self.test_size)

#         stratify_temp = y_temp if len(np.unique(y_temp)) > 1 else None
#         X_train, X_val, y_train, y_val = train_test_split(
#             X_temp, y_temp,
#             test_size=val_size_adjusted,
#             stratify=stratify_temp,
#             random_state=self.random_state
#         )

#         # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…
#         return X_train, X_val, X_test, y_train, y_val, y_test
    
#     def get_split_info(self, y_train, y_val, y_test) -> Dict:
#         """
#         Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ù† ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø³ÙŠÙ…
#         """
#         def get_distribution(y):
#             unique, counts = np.unique(y, return_counts=True)
#             return dict(zip(unique, counts))
        
#         return {
#             'train_samples': len(y_train),
#             'val_samples': len(y_val),
#             'test_samples': len(y_test),
#             'train_distribution': get_distribution(y_train),
#             'val_distribution': get_distribution(y_val),
#             'test_distribution': get_distribution(y_test),
#         }
